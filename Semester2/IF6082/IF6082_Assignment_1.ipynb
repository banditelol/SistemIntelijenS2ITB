{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IF6082 Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kAqB_y2zCPOw"
      ],
      "authorship_tag": "ABX9TyMMSg9E4kwk3INBKmn7sNuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banditelol/SistemIntelijenS2ITB/blob/master/Semester2/IF6082/IF6082_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyTHxHYO_iB0"
      },
      "source": [
        "# Assignment 1\r\n",
        "\r\n",
        "Buatlah kode program penggunaan NLTK atau Spacy untuk\r\n",
        "1. Sentence splitter\r\n",
        "2. Tokenization\r\n",
        "3. Stemming\r\n",
        "4. Lemmatization\r\n",
        "5. Entity Masking\r\n",
        "6. POS Tagger\r\n",
        "7. Phrase Chunking\r\n",
        "\r\n",
        "Dibuat dalam file `.ipynb` atau `.py` dimana setiap kode program perlu diberi komentar berupa penjelasan kode program tersebut, nama file adalah nim mahasiswa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqB_y2zCPOw"
      },
      "source": [
        "## Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0scdQkrrCMB0"
      },
      "source": [
        "class Blank:\r\n",
        "  def __init__(self):\r\n",
        "    pass\r\n",
        "  def stem(self, token):\r\n",
        "    return token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PU9EXY_79h"
      },
      "source": [
        "## Asumsi\r\n",
        "\r\n",
        "Text yang digunakan diambil dari article thegradient.pub [tentang Timmit Gebru](https://thegradient.pub/the-far-reaching-impacts-of-timnit-gebru/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9_kW2PhArdg",
        "outputId": "a3700b07-4e7a-42b4-ce45-2e195b10c2d7"
      },
      "source": [
        "text = r\"Few researchers make breakthrough contributions to even a single field. Fewer still can claim to have made breakthrough contributions to multiple fields. Dr. Timnit Gebru is one of those few. She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more. Many of her papers have been cited hundreds of times.\"\r\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few researchers make breakthrough contributions to even a single field. Fewer still can claim to have made breakthrough contributions to multiple fields. Dr. Timnit Gebru is one of those few. She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more. Many of her papers have been cited hundreds of times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBtj5kNr_6Wm"
      },
      "source": [
        "# NLTK\r\n",
        "\r\n",
        "## Setup NLTK\r\n",
        "\r\n",
        "Selain import NLTK, dibutuhkan beberapa package tambahan untuk bisa melakukan task-task yang diberikan pada assignment ini, salah satunya\r\n",
        "\r\n",
        "### Punkt tokenizer\r\n",
        "\r\n",
        "Berdasarkan [dokumentasi NLTK](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt)\r\n",
        "\r\n",
        "> This (punkt) tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToW-T-BF--3a",
        "outputId": "98e34495-a4a2-4d17-e1f3-f4b286645bf0"
      },
      "source": [
        "import nltk\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "# Package punkt dibutuhkan oleh nltk untuk melakukan sentence tokenizer\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjEQ6UTEuMQ"
      },
      "source": [
        "## Sentence Splitter\r\n",
        "\r\n",
        "Untuk sentence splitter pada nltk digunakan fungsi sent_tokenize dari [package tokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize). sent_tokenize ini secara default menggunakan [punkt](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2006.32.4.485), dan kita juga bisa mendefinisikan mau melakukan tokenizer berdasarkan [corpus bahasa](https://github.com/nltk/nltk_data/blob/gh-pages/packages/tokenizers/punkt.xml) yang tersedia di NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji37_Md9EAdJ",
        "outputId": "f5e0186e-6606-45c1-e3bf-87970533beb7"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\r\n",
        "\r\n",
        "sent_token = sent_tokenize(text)\r\n",
        "for i, sent in enumerate(sent_token) :\r\n",
        "  print(f\"kalimat ke-{i}: {sent}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kalimat ke-0: Few researchers make breakthrough contributions to even a single field.\n",
            "kalimat ke-1: Fewer still can claim to have made breakthrough contributions to multiple fields.\n",
            "kalimat ke-2: Dr. Timnit Gebru is one of those few.\n",
            "kalimat ke-3: She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more.\n",
            "kalimat ke-4: Many of her papers have been cited hundreds of times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdOis1zsMv0s"
      },
      "source": [
        "## Tokenization (Word Tokenize)\r\n",
        "\r\n",
        "Setelah membagi kalimat, kita bisa melakukan tokenizer terhadap masing-masing kata dengan menggunakan `word_tokenize`. Walaupun practically `word_tokenize` pada NLTK akan melakukan `sent_tokenize` [terlebih dahulu](https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize) pada input text.\r\n",
        "\r\n",
        "Berbeda dengan `sent_tokenize` yang menggunakan latihan unsupervised untuk menghasilkan model tokenizernya, `word_tokenize` [menggunakan `regex`](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.treebank.TreebankWordTokenizer) untuk menghasilkan token seperti pada Penn Treebank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wv7RR0uOc1R",
        "outputId": "b19b294c-4591-4110-ec93-cc2a0f2ad2ef"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "word_token = [nltk.word_tokenize(sent) for sent in sent_token]\r\n",
        "for words in word_token:\r\n",
        "  print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAMymmGNPekx"
      },
      "source": [
        "## Stemming\r\n",
        "\r\n",
        "Stemming merupakan proses menghilangkan  `morphological affixes` dari suatu kata sehingga meninggalkan _stem_ (akar)nya saja. Dalam NLTK hal ini sudah diberikan dalam modul [`nltk.stem`](https://www.nltk.org/api/nltk.stem.html). Didalamnya terdapat beberapa jenis stemming yang bisa dilakukan untuk beberapa pilihan bahasa. \r\n",
        "\r\n",
        "Untuk bahasa inggris khususnya terdapat [Porter Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) yang merupakan stemmer berdasarkan paper original Porter (_Porter, M. “An algorithm for suffix stripping.” Program 14.3 (1980): 130-137._) secara default menggunakan versi yang telah dimodifikasi oleh kontributor NLTK.\r\n",
        "\r\n",
        "Terdapat juga [Lancaster Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.lancaster) berdasarkan paper _Paice, Chris D. “Another Stemmer.” ACM SIGIR Forum 24.3 (1990): 56-61_.\r\n",
        "\r\n",
        "Dan terakhir terdapat [Snowball Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball) yang dikenal juga sebagai _Porter2_ dan secara umum dinilai sebagai improvement dari algoritma _Porter_, bahkan oleh Porter sendiri (yang juga menjadi maintainer hingga 2014).\r\n",
        "\r\n",
        "Pembahasan pendek mengenai perbedaannya secara praktis bisa dilihat di [stackoverflow](https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd_VSsuSZtST",
        "outputId": "2b807e90-c385-49c3-b0e5-74e8786f5f10"
      },
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer \r\n",
        "from nltk.stem.snowball import EnglishStemmer\r\n",
        "\r\n",
        "stemmers = {\"original\":Blank, \"porter\": PorterStemmer,  \"porter2\":EnglishStemmer, \"lancaster\":LancasterStemmer}\r\n",
        "for words in word_token:\r\n",
        "  for name, St in stemmers.items():\r\n",
        "    st = St()\r\n",
        "    print(f\"{name:<10}: {[st.stem(word) for word in words]}\")\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original  : ['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "porter    : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "porter2   : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "lancaster : ['few', 'research', 'mak', 'breakthrough', 'contribut', 'to', 'ev', 'a', 'singl', 'field', '.']\n",
            "\n",
            "original  : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "porter    : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "porter2   : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "lancaster : ['few', 'stil', 'can', 'claim', 'to', 'hav', 'mad', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "\n",
            "original  : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "porter    : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "porter2   : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "lancaster : ['dr.', 'timnit', 'gebru', 'is', 'on', 'of', 'thos', 'few', '.']\n",
            "\n",
            "original  : ['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "porter    : ['she', 'ha', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'AI', 'account', ';', 'and', 'more', '.']\n",
            "porter2   : ['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "lancaster : ['she', 'has', 'work', 'on', 'comput', 'vis', 'problem', 'in', 'fine-grained', 'object', 'recognit', ';', 'us', 'large-scal', 'im', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduc', 'audit', 'of', 'bias', 'fac', 'recognit', 'system', 'which', 'hav', 'influ', 'real-world', 'reg', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'eth', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'mor', '.']\n",
            "\n",
            "original  : ['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n",
            "porter    : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "porter2   : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "lancaster : ['many', 'of', 'her', 'pap', 'hav', 'been', 'cit', 'hundr', 'of', 'tim', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG7BbL73drZh"
      },
      "source": [
        "Bisa diamati bahwa porter dengan modifikasi dari tim NLTK dan porter2 (snowball) menghasilkan stem yang mirip (salah satu perbedaan yang terlihat baru di kata `has` yang berubah menjadi `ha` dengan menggunakan porter. Sedangkan lancaster menghasilkan kata yang lebih pendek, bahkan hingga sulit mengetahui kata sebelum stemming (seperti `paper` menjadi `pap`). \r\n",
        "\r\n",
        "Untuk assignment ini **saya memilih menggunakan porter2** (`SnowballStemmer`) untuk task-task selanjutnya, karena:\r\n",
        "- Porter sendiri merekomendasikan metode ini dibanding metode originalnya\r\n",
        "- Lancaster terlalu agresif dalam melakukan stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlwaTIbrdrJm",
        "outputId": "e8138051-961c-43c3-9d17-ac7d48fbe376"
      },
      "source": [
        "st = EnglishStemmer()\r\n",
        "stemmed_token = [[st.stem(word) for word in words] for words in word_token]\r\n",
        "\r\n",
        "for stemmed in stemmed_token:\r\n",
        "  print(stemmed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoUkejW4imsW"
      },
      "source": [
        "## Lemmatization\r\n",
        "\r\n",
        "Bila Stemming biasanya menghasilkan kata yang _terpotong_ dan bisa jadi tidak ada di kamus bahasa tersebut, lemmatization berusaha untuk menghasilkan kata asal yang memang ada di kamus suatu bahasa. di NLTK ini hanya ada satu metode lemmatization, yaitu menggunakan wordnet. \r\n",
        "\r\n",
        "Untuk bisa menggunakan wordnet, terlebih dahulu perlu kita download dulu modul wordnet dari nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85OyrMR7rZGt",
        "outputId": "dd5042b9-d7a2-4417-a20f-f5fcb108d6d4"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHOTjF1nimDR",
        "outputId": "1c951354-5240-4ae1-90b9-1b4772ad1421"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "lemmatized_token = [[wnl.lemmatize(word) for word in words] for words in word_token]\r\n",
        "\r\n",
        "for original, stemmed, lemmatized in zip(word_token, stemmed_token, lemmatized_token):\r\n",
        "  print(f\"{'original':<10} : {original}\")\r\n",
        "  print(f\"{'lemmatized':<10} : {lemmatized}\")\r\n",
        "  print(f\"{'stemmed':<10} : {stemmed}\")\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original   : ['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "lemmatized : ['Few', 'researcher', 'make', 'breakthrough', 'contribution', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "stemmed    : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "\n",
            "original   : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "lemmatized : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribution', 'to', 'multiple', 'field', '.']\n",
            "stemmed    : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "\n",
            "original   : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "lemmatized : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "stemmed    : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "\n",
            "original   : ['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "lemmatized : ['She', 'ha', 'worked', 'on', 'computer', 'vision', 'problem', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'set', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audit', 'of', 'biased', 'facial', 'recognition', 'system', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standard', 'and', 'process', 'to', 'mitigate', 'ethical', 'issue', 'with', 'datasets', 'and', 'model', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audit', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "stemmed    : ['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "\n",
            "original   : ['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n",
            "lemmatized : ['Many', 'of', 'her', 'paper', 'have', 'been', 'cited', 'hundred', 'of', 'time', '.']\n",
            "stemmed    : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FiVhWnTrztr"
      },
      "source": [
        "## Entity Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C94vG15Qr2pN"
      },
      "source": [
        "## POS Tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajU-y1qcr4go"
      },
      "source": [
        "## Phrase Chunking"
      ]
    }
  ]
}