{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IF6082 Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNR/SqpzkdHV+6mw0zYbL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banditelol/SistemIntelijenS2ITB/blob/master/Semester2/IF6082/IF6082_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyTHxHYO_iB0"
      },
      "source": [
        "# Assignment 1\r\n",
        "\r\n",
        "Buatlah kode program penggunaan NLTK atau Spacy untuk\r\n",
        "1. Sentence splitter\r\n",
        "2. Tokenization\r\n",
        "3. Stemming\r\n",
        "4. Lemmatization\r\n",
        "5. Entity Masking\r\n",
        "6. POS Tagger\r\n",
        "7. Phrase Chunking\r\n",
        "\r\n",
        "Dibuat dalam file `.ipynb` atau `.py` dimana setiap kode program perlu diberi komentar berupa penjelasan kode program tersebut, nama file adalah nim mahasiswa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqB_y2zCPOw"
      },
      "source": [
        "## Helper Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0scdQkrrCMB0"
      },
      "source": [
        "class Blank:\r\n",
        "  def __init__(self):\r\n",
        "    pass\r\n",
        "  def stem(self, token):\r\n",
        "    return token\r\n",
        "\r\n",
        "from nltk.corpus import wordnet as wn\r\n",
        "\r\n",
        "def penn2morphy(penntag, returnNoun=False):\r\n",
        "    morphy_tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\r\n",
        "                  'VB':wn.VERB, 'RB':wn.ADV}\r\n",
        "    try:\r\n",
        "        return morphy_tag[penntag[:2]]\r\n",
        "    except:\r\n",
        "        return wn.NOUN if returnNoun else ''"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PU9EXY_79h"
      },
      "source": [
        "## Asumsi\r\n",
        "\r\n",
        "Text yang digunakan diambil dari article thegradient.pub [tentang Timmit Gebru](https://thegradient.pub/the-far-reaching-impacts-of-timnit-gebru/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9_kW2PhArdg",
        "outputId": "bf2a0a9c-0f87-40c9-ab8b-0fe5d39b8989"
      },
      "source": [
        "text = r\"Few researchers make breakthrough contributions to even a single field. Fewer still can claim to have made breakthrough contributions to multiple fields. Dr. Timnit Gebru is one of those few. She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more. Many of her papers have been cited hundreds of times.\"\r\n",
        "print(text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few researchers make breakthrough contributions to even a single field. Fewer still can claim to have made breakthrough contributions to multiple fields. Dr. Timnit Gebru is one of those few. She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more. Many of her papers have been cited hundreds of times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBtj5kNr_6Wm"
      },
      "source": [
        "# NLTK\r\n",
        "\r\n",
        "## Setup NLTK\r\n",
        "\r\n",
        "Selain import NLTK, dibutuhkan beberapa package tambahan untuk bisa melakukan task-task yang diberikan pada assignment ini, salah satunya\r\n",
        "\r\n",
        "### Punkt tokenizer\r\n",
        "\r\n",
        "Berdasarkan [dokumentasi NLTK](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt)\r\n",
        "\r\n",
        "> This (punkt) tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToW-T-BF--3a",
        "outputId": "a6c3f4ae-e73e-4960-e64f-31150f82e119"
      },
      "source": [
        "import nltk\r\n",
        "from pandas import DataFrame\r\n",
        "\r\n",
        "# Package punkt dibutuhkan oleh nltk untuk melakukan sentence tokenizer\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdjEQ6UTEuMQ"
      },
      "source": [
        "## Sentence Splitter\r\n",
        "\r\n",
        "Untuk sentence splitter pada nltk digunakan fungsi sent_tokenize dari [package tokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.sent_tokenize). sent_tokenize ini secara default menggunakan [punkt](https://www.mitpressjournals.org/doi/pdf/10.1162/coli.2006.32.4.485), dan kita juga bisa mendefinisikan mau melakukan tokenizer berdasarkan [corpus bahasa](https://github.com/nltk/nltk_data/blob/gh-pages/packages/tokenizers/punkt.xml) yang tersedia di NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji37_Md9EAdJ",
        "outputId": "c89e189e-25e3-42f4-ac32-5e65ab4bf72d"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\r\n",
        "\r\n",
        "sent_token = sent_tokenize(text)\r\n",
        "for i, sent in enumerate(sent_token) :\r\n",
        "  print(f\"kalimat ke-{i}: {sent}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kalimat ke-0: Few researchers make breakthrough contributions to even a single field.\n",
            "kalimat ke-1: Fewer still can claim to have made breakthrough contributions to multiple fields.\n",
            "kalimat ke-2: Dr. Timnit Gebru is one of those few.\n",
            "kalimat ke-3: She has worked on computer vision problems in fine-grained object recognition; used large-scale image sets to gain sociological insight; conducted audits of biased facial recognition systems which have influenced real-world regulation; designed standards and processes to mitigate ethical issues with datasets and models; developed a framework of algorithmic audits for AI accountability; and more.\n",
            "kalimat ke-4: Many of her papers have been cited hundreds of times.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdOis1zsMv0s"
      },
      "source": [
        "## Tokenization (Word Tokenize)\r\n",
        "\r\n",
        "Setelah membagi kalimat, kita bisa melakukan tokenizer terhadap masing-masing kata dengan menggunakan `word_tokenize`. Walaupun practically `word_tokenize` pada NLTK akan melakukan `sent_tokenize` [terlebih dahulu](https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize) pada input text.\r\n",
        "\r\n",
        "Berbeda dengan `sent_tokenize` yang menggunakan latihan unsupervised untuk menghasilkan model tokenizernya, `word_tokenize` [menggunakan `regex`](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.treebank.TreebankWordTokenizer) untuk menghasilkan token seperti pada Penn Treebank.\r\n",
        "\r\n",
        "Setelah coba eksplorasi NLTK lagi, ternyata cukup banyak jenis tokenizer yang disediakan oleh NLTK:\r\n",
        "- Stanford\r\n",
        "- Word E"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wv7RR0uOc1R",
        "outputId": "ea35b2b0-e79f-40b6-dbb3-9cdebfa40eaf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\r\n",
        "\r\n",
        "word_token = [nltk.word_tokenize(sent) for sent in sent_token]\r\n",
        "for words in word_token:\r\n",
        "  print(words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAMymmGNPekx"
      },
      "source": [
        "## Stemming\r\n",
        "\r\n",
        "Stemming merupakan proses menghilangkan  `morphological affixes` dari suatu kata sehingga meninggalkan _stem_ (akar)nya saja. Dalam NLTK hal ini sudah diberikan dalam modul [`nltk.stem`](https://www.nltk.org/api/nltk.stem.html). Didalamnya terdapat beberapa jenis stemming yang bisa dilakukan untuk beberapa pilihan bahasa. \r\n",
        "\r\n",
        "Untuk bahasa inggris khususnya terdapat [Porter Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) yang merupakan stemmer berdasarkan paper original Porter (_Porter, M. “An algorithm for suffix stripping.” Program 14.3 (1980): 130-137._) secara default menggunakan versi yang telah dimodifikasi oleh kontributor NLTK.\r\n",
        "\r\n",
        "Terdapat juga [Lancaster Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.lancaster) berdasarkan paper _Paice, Chris D. “Another Stemmer.” ACM SIGIR Forum 24.3 (1990): 56-61_.\r\n",
        "\r\n",
        "Dan terakhir terdapat [Snowball Stemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball) yang dikenal juga sebagai _Porter2_ dan secara umum dinilai sebagai improvement dari algoritma _Porter_, bahkan oleh Porter sendiri (yang juga menjadi maintainer hingga 2014).\r\n",
        "\r\n",
        "Pembahasan pendek mengenai perbedaannya secara praktis bisa dilihat di [stackoverflow](https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd_VSsuSZtST",
        "outputId": "81f1de20-6121-4f5d-e595-b56341ae3340"
      },
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer \r\n",
        "from nltk.stem.snowball import EnglishStemmer\r\n",
        "\r\n",
        "stemmers = {\"original\":Blank, \"porter\": PorterStemmer,  \"porter2\":EnglishStemmer, \"lancaster\":LancasterStemmer}\r\n",
        "for words in word_token:\r\n",
        "  for name, St in stemmers.items():\r\n",
        "    st = St()\r\n",
        "    print(f\"{name:<10}: {[st.stem(word) for word in words]}\")\r\n",
        "  print()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original  : ['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "porter    : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "porter2   : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "lancaster : ['few', 'research', 'mak', 'breakthrough', 'contribut', 'to', 'ev', 'a', 'singl', 'field', '.']\n",
            "\n",
            "original  : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "porter    : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "porter2   : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "lancaster : ['few', 'stil', 'can', 'claim', 'to', 'hav', 'mad', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "\n",
            "original  : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "porter    : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "porter2   : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "lancaster : ['dr.', 'timnit', 'gebru', 'is', 'on', 'of', 'thos', 'few', '.']\n",
            "\n",
            "original  : ['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "porter    : ['she', 'ha', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'AI', 'account', ';', 'and', 'more', '.']\n",
            "porter2   : ['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "lancaster : ['she', 'has', 'work', 'on', 'comput', 'vis', 'problem', 'in', 'fine-grained', 'object', 'recognit', ';', 'us', 'large-scal', 'im', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduc', 'audit', 'of', 'bias', 'fac', 'recognit', 'system', 'which', 'hav', 'influ', 'real-world', 'reg', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'eth', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'mor', '.']\n",
            "\n",
            "original  : ['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n",
            "porter    : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "porter2   : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "lancaster : ['many', 'of', 'her', 'pap', 'hav', 'been', 'cit', 'hundr', 'of', 'tim', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG7BbL73drZh"
      },
      "source": [
        "Bisa diamati bahwa porter dengan modifikasi dari tim NLTK dan porter2 (snowball) menghasilkan stem yang mirip (salah satu perbedaan yang terlihat baru di kata `has` yang berubah menjadi `ha` dengan menggunakan porter. Sedangkan lancaster menghasilkan kata yang lebih pendek, bahkan hingga sulit mengetahui kata sebelum stemming (seperti `paper` menjadi `pap`). \r\n",
        "\r\n",
        "Untuk assignment ini **saya memilih menggunakan porter2** (`SnowballStemmer`) untuk task-task selanjutnya, karena:\r\n",
        "- Porter sendiri merekomendasikan metode ini dibanding metode originalnya\r\n",
        "- Lancaster terlalu agresif dalam melakukan stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlwaTIbrdrJm",
        "outputId": "1bfc882e-f7f7-4d6e-9f1a-fd69d839f6c8"
      },
      "source": [
        "st = EnglishStemmer()\r\n",
        "stemmed_token = [[st.stem(word) for word in words] for words in word_token]\r\n",
        "\r\n",
        "for stemmed in stemmed_token:\r\n",
        "  print(stemmed)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoUkejW4imsW"
      },
      "source": [
        "## Lemmatization\r\n",
        "\r\n",
        "Bila Stemming biasanya menghasilkan kata yang _terpotong_ dan bisa jadi tidak ada di kamus bahasa tersebut, lemmatization berusaha untuk menghasilkan kata asal yang memang ada di kamus suatu bahasa. di NLTK ini hanya ada satu metode lemmatization, yaitu menggunakan wordnet. \r\n",
        "\r\n",
        "Untuk bisa menggunakan wordnet, terlebih dahulu perlu kita download dulu modul wordnet dari nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85OyrMR7rZGt",
        "outputId": "e4e30aa7-c12b-4613-9ef8-332ef6f7e154"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHOTjF1nimDR",
        "outputId": "a25a0acc-aa90-42cd-db51-4291b8ac78e5"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "wnl = WordNetLemmatizer()\r\n",
        "lemmatized_token = [[wnl.lemmatize(word) for word in words] for words in word_token]\r\n",
        "\r\n",
        "for original, stemmed, lemmatized in zip(word_token, stemmed_token, lemmatized_token):\r\n",
        "  print(f\"{'original':<10} : {original}\")\r\n",
        "  print(f\"{'lemmatized':<10} : {lemmatized}\")\r\n",
        "  print(f\"{'stemmed':<10} : {stemmed}\")\r\n",
        "  print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original   : ['Few', 'researchers', 'make', 'breakthrough', 'contributions', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "lemmatized : ['Few', 'researcher', 'make', 'breakthrough', 'contribution', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "stemmed    : ['few', 'research', 'make', 'breakthrough', 'contribut', 'to', 'even', 'a', 'singl', 'field', '.']\n",
            "\n",
            "original   : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contributions', 'to', 'multiple', 'fields', '.']\n",
            "lemmatized : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribution', 'to', 'multiple', 'field', '.']\n",
            "stemmed    : ['fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribut', 'to', 'multipl', 'field', '.']\n",
            "\n",
            "original   : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "lemmatized : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "stemmed    : ['dr.', 'timnit', 'gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "\n",
            "original   : ['She', 'has', 'worked', 'on', 'computer', 'vision', 'problems', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'sets', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audits', 'of', 'biased', 'facial', 'recognition', 'systems', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standards', 'and', 'processes', 'to', 'mitigate', 'ethical', 'issues', 'with', 'datasets', 'and', 'models', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audits', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "lemmatized : ['She', 'ha', 'worked', 'on', 'computer', 'vision', 'problem', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'set', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audit', 'of', 'biased', 'facial', 'recognition', 'system', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standard', 'and', 'process', 'to', 'mitigate', 'ethical', 'issue', 'with', 'datasets', 'and', 'model', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audit', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "stemmed    : ['she', 'has', 'work', 'on', 'comput', 'vision', 'problem', 'in', 'fine-grain', 'object', 'recognit', ';', 'use', 'large-scal', 'imag', 'set', 'to', 'gain', 'sociolog', 'insight', ';', 'conduct', 'audit', 'of', 'bias', 'facial', 'recognit', 'system', 'which', 'have', 'influenc', 'real-world', 'regul', ';', 'design', 'standard', 'and', 'process', 'to', 'mitig', 'ethic', 'issu', 'with', 'dataset', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithm', 'audit', 'for', 'ai', 'account', ';', 'and', 'more', '.']\n",
            "\n",
            "original   : ['Many', 'of', 'her', 'papers', 'have', 'been', 'cited', 'hundreds', 'of', 'times', '.']\n",
            "lemmatized : ['Many', 'of', 'her', 'paper', 'have', 'been', 'cited', 'hundred', 'of', 'time', '.']\n",
            "stemmed    : ['mani', 'of', 'her', 'paper', 'have', 'been', 'cite', 'hundr', 'of', 'time', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FiVhWnTrztr"
      },
      "source": [
        "## Entity Masking\r\n",
        "\r\n",
        "Untuk Entity Masking ini bisa dilakukan dengan menggunakan regex matching. Atau dengan kata lain, untuk melakukan masking ini kita berasumsi bahwa kita sudah tau pattern entity yang ingin di masking terlebih dahulu. Berikut adalah contoh penggunaan entity masking untuk melakukan masking terhadap alamat email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqo1gqKqfGCM",
        "outputId": "5979063c-cbbb-47ab-fa42-caf5843a5749"
      },
      "source": [
        "import re\r\n",
        "email = re.compile('\\w+@\\w+.[a-z]{3}')\r\n",
        "text = \"Silahkan kirim email lanjutan ke alamat email help@domain.com atau gunakan alamat lama di bantuan@domain.com\"\r\n",
        "text_masked = email.sub('_email_', text)\r\n",
        "print(text_masked)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silahkan kirim email lanjutan ke alamat email _email_ atau gunakan alamat lama di _email_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UEWE4n4f4Pi"
      },
      "source": [
        "Namun kekurangan dari metode ini adalah kemampuan masking sangat bergantung pada rule yang diberikan. Misalkan untuk contoh di atas bila kita memberikan masukan dengan TLD yang tidak 3 huruf maka alamat email tsb tidak akan melakukan masking dengan baik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHRx816FgJB2",
        "outputId": "aeba2e1f-03ef-4866-c93b-8f919f0a2b81"
      },
      "source": [
        "print(email.sub('_email_',\"23520032@std.stei.itb.ac.id\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_email_i.itb.ac.id\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C94vG15Qr2pN"
      },
      "source": [
        "## POS Tagger\r\n",
        "\r\n",
        "[[Penn Treebank Tagset](https://www.sketchengine.eu/penn-treebank-tagset/)] [[Penn Treebank Tagset (Alt)](https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html)] [[Definisi Proper Noun](https://www.grammarly.com/blog/proper-nouns/)] [[Tagger atau Stemmer dulu](https://stackoverflow.com/questions/27227111/confused-about-priority-between-stemmer-and-pos-tagger)]\r\n",
        "\r\n",
        "dalam NLTK POS Tagging bisa dilakukan dengan menggunakan beberapa metode, diantaranya menggunakan [CRFSuite](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.crf), [Hunpos](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.hunpos), [Stanford](https://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford), maupun dengan menggunakan pos tagger rekomendasi dari NLTK [`pos_tag`](https://www.nltk.org/api/nltk.tag.html?highlight=pos_tag#module-nltk.tag). \r\n",
        "\r\n",
        "Dengan menggunakan `pos_tag`, dengan default untuk [bahasa inggris](https://github.com/nltk/nltk/blob/2f81487fe11e829d4b7d578af0ecf1c8a69a5c79/nltk/tag/__init__.py#L100-L107) akan menggunakan PerceptronTagger yang berdasarkan Greedy Averaged Perceptron oleh [Matthew Honnibal](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python). Untuk menggunakan Average Perceptron Tagger kita perlu download terlebih dahulu menggunakan `nltk.download`.\r\n",
        "\r\n",
        "untuk text yang terdiri dari beberapa kalimat disarankan untuk langsung menggunakan `pos_tag_sents` dibandingkan menggunakan `pos_tag` untuk masing-masing kalimat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuGVtKwNgZQj",
        "outputId": "134505bf-cc4e-4075-b326-0ff718f0fbe2"
      },
      "source": [
        "from nltk import pos_tag, pos_tag_sents\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "\r\n",
        "print()\r\n",
        "\r\n",
        "tagged_token = pos_tag_sents(word_token)\r\n",
        "for words, tagged in zip(word_token, tagged_token):\r\n",
        "  print(f\"{'pos_tag':<15}:\",pos_tag(words))\r\n",
        "  print(f\"{'pos_tag_sents':<15}:\",tagged)\r\n",
        "  print()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "\n",
            "pos_tag        : [('Few', 'JJ'), ('researchers', 'NNS'), ('make', 'VBP'), ('breakthrough', 'JJ'), ('contributions', 'NNS'), ('to', 'TO'), ('even', 'VB'), ('a', 'DT'), ('single', 'JJ'), ('field', 'NN'), ('.', '.')]\n",
            "pos_tag_sents  : [('Few', 'JJ'), ('researchers', 'NNS'), ('make', 'VBP'), ('breakthrough', 'JJ'), ('contributions', 'NNS'), ('to', 'TO'), ('even', 'VB'), ('a', 'DT'), ('single', 'JJ'), ('field', 'NN'), ('.', '.')]\n",
            "\n",
            "pos_tag        : [('Fewer', 'JJR'), ('still', 'RB'), ('can', 'MD'), ('claim', 'VB'), ('to', 'TO'), ('have', 'VB'), ('made', 'VBN'), ('breakthrough', 'IN'), ('contributions', 'NNS'), ('to', 'TO'), ('multiple', 'VB'), ('fields', 'NNS'), ('.', '.')]\n",
            "pos_tag_sents  : [('Fewer', 'JJR'), ('still', 'RB'), ('can', 'MD'), ('claim', 'VB'), ('to', 'TO'), ('have', 'VB'), ('made', 'VBN'), ('breakthrough', 'IN'), ('contributions', 'NNS'), ('to', 'TO'), ('multiple', 'VB'), ('fields', 'NNS'), ('.', '.')]\n",
            "\n",
            "pos_tag        : [('Dr.', 'NNP'), ('Timnit', 'NNP'), ('Gebru', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('those', 'DT'), ('few', 'JJ'), ('.', '.')]\n",
            "pos_tag_sents  : [('Dr.', 'NNP'), ('Timnit', 'NNP'), ('Gebru', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('those', 'DT'), ('few', 'JJ'), ('.', '.')]\n",
            "\n",
            "pos_tag        : [('She', 'PRP'), ('has', 'VBZ'), ('worked', 'VBN'), ('on', 'IN'), ('computer', 'NN'), ('vision', 'NN'), ('problems', 'NNS'), ('in', 'IN'), ('fine-grained', 'JJ'), ('object', 'JJ'), ('recognition', 'NN'), (';', ':'), ('used', 'VBN'), ('large-scale', 'JJ'), ('image', 'NN'), ('sets', 'NNS'), ('to', 'TO'), ('gain', 'VB'), ('sociological', 'JJ'), ('insight', 'NN'), (';', ':'), ('conducted', 'VBN'), ('audits', 'NNS'), ('of', 'IN'), ('biased', 'JJ'), ('facial', 'JJ'), ('recognition', 'NN'), ('systems', 'NNS'), ('which', 'WDT'), ('have', 'VBP'), ('influenced', 'VBN'), ('real-world', 'JJ'), ('regulation', 'NN'), (';', ':'), ('designed', 'VBN'), ('standards', 'NNS'), ('and', 'CC'), ('processes', 'NNS'), ('to', 'TO'), ('mitigate', 'VB'), ('ethical', 'JJ'), ('issues', 'NNS'), ('with', 'IN'), ('datasets', 'NNS'), ('and', 'CC'), ('models', 'NNS'), (';', ':'), ('developed', 'VBD'), ('a', 'DT'), ('framework', 'NN'), ('of', 'IN'), ('algorithmic', 'JJ'), ('audits', 'NNS'), ('for', 'IN'), ('AI', 'NNP'), ('accountability', 'NN'), (';', ':'), ('and', 'CC'), ('more', 'RBR'), ('.', '.')]\n",
            "pos_tag_sents  : [('She', 'PRP'), ('has', 'VBZ'), ('worked', 'VBN'), ('on', 'IN'), ('computer', 'NN'), ('vision', 'NN'), ('problems', 'NNS'), ('in', 'IN'), ('fine-grained', 'JJ'), ('object', 'JJ'), ('recognition', 'NN'), (';', ':'), ('used', 'VBN'), ('large-scale', 'JJ'), ('image', 'NN'), ('sets', 'NNS'), ('to', 'TO'), ('gain', 'VB'), ('sociological', 'JJ'), ('insight', 'NN'), (';', ':'), ('conducted', 'VBN'), ('audits', 'NNS'), ('of', 'IN'), ('biased', 'JJ'), ('facial', 'JJ'), ('recognition', 'NN'), ('systems', 'NNS'), ('which', 'WDT'), ('have', 'VBP'), ('influenced', 'VBN'), ('real-world', 'JJ'), ('regulation', 'NN'), (';', ':'), ('designed', 'VBN'), ('standards', 'NNS'), ('and', 'CC'), ('processes', 'NNS'), ('to', 'TO'), ('mitigate', 'VB'), ('ethical', 'JJ'), ('issues', 'NNS'), ('with', 'IN'), ('datasets', 'NNS'), ('and', 'CC'), ('models', 'NNS'), (';', ':'), ('developed', 'VBD'), ('a', 'DT'), ('framework', 'NN'), ('of', 'IN'), ('algorithmic', 'JJ'), ('audits', 'NNS'), ('for', 'IN'), ('AI', 'NNP'), ('accountability', 'NN'), (';', ':'), ('and', 'CC'), ('more', 'RBR'), ('.', '.')]\n",
            "\n",
            "pos_tag        : [('Many', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('papers', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('cited', 'VBN'), ('hundreds', 'NNS'), ('of', 'IN'), ('times', 'NNS'), ('.', '.')]\n",
            "pos_tag_sents  : [('Many', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('papers', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('cited', 'VBN'), ('hundreds', 'NNS'), ('of', 'IN'), ('times', 'NNS'), ('.', '.')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAFxqPI94adj"
      },
      "source": [
        "Bisa dilihat bahwa masukan dari pos tagger adalah token yang tidak diubah baik menggunakan lemmatization maupun stemming. Asumsi saya ini dilakukan karena penggunaan Perceptron Tagger dimana dalam trainingnya preprocessing terhadap kalimat hanya dilakukan [mengikuti](https://www.nltk.org/api/nltk.tag.html#nltk.tag.perceptron.PerceptronTagger.normalize) [blog post sumbernya](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python#features-and-pre-processing), yaitu:\r\n",
        "\r\n",
        "- All words are lower cased;\r\n",
        "- Digits in the range 1800-2100 are represented as !YEAR;\r\n",
        "- Other digit strings are represented as !DIGITS\r\n",
        "- Word with Hyphen (-) represented as !HYPHEN\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u8F1afK7WMD"
      },
      "source": [
        "### Tagged Token sebagai Input Lemmatization\r\n",
        "\r\n",
        "Salah satu hal yang saya dengar dari Fairuz dan juga saya amati di dokumentasi NLTK, bahwa lematisasi bisa dilakukan sembari memberikan informasi mengenai Tag dari token tersebut, dan katanya menghasilkan lemma yang lebih akurat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VpfkQYH7Vjs",
        "outputId": "08e464ff-b168-490e-eb64-eb84e1c861d0"
      },
      "source": [
        "lemma_tagged = [[wnl.lemmatize(word, pos=penn2morphy(tag, returnNoun=True)) for word,tag in tagged_words] for tagged_words in tagged_token]\r\n",
        "\r\n",
        "for lemma, tagged in zip(lemmatized_token, lemma_tagged):\r\n",
        "  print(f\"{'Original':<15}:\",lemma)\r\n",
        "  print(f\"{'with POS':<15}:\",tagged)\r\n",
        "  print()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original       : ['Few', 'researcher', 'make', 'breakthrough', 'contribution', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "with POS       : ['Few', 'researcher', 'make', 'breakthrough', 'contribution', 'to', 'even', 'a', 'single', 'field', '.']\n",
            "\n",
            "Original       : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'made', 'breakthrough', 'contribution', 'to', 'multiple', 'field', '.']\n",
            "with POS       : ['Fewer', 'still', 'can', 'claim', 'to', 'have', 'make', 'breakthrough', 'contribution', 'to', 'multiple', 'field', '.']\n",
            "\n",
            "Original       : ['Dr.', 'Timnit', 'Gebru', 'is', 'one', 'of', 'those', 'few', '.']\n",
            "with POS       : ['Dr.', 'Timnit', 'Gebru', 'be', 'one', 'of', 'those', 'few', '.']\n",
            "\n",
            "Original       : ['She', 'ha', 'worked', 'on', 'computer', 'vision', 'problem', 'in', 'fine-grained', 'object', 'recognition', ';', 'used', 'large-scale', 'image', 'set', 'to', 'gain', 'sociological', 'insight', ';', 'conducted', 'audit', 'of', 'biased', 'facial', 'recognition', 'system', 'which', 'have', 'influenced', 'real-world', 'regulation', ';', 'designed', 'standard', 'and', 'process', 'to', 'mitigate', 'ethical', 'issue', 'with', 'datasets', 'and', 'model', ';', 'developed', 'a', 'framework', 'of', 'algorithmic', 'audit', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "with POS       : ['She', 'have', 'work', 'on', 'computer', 'vision', 'problem', 'in', 'fine-grained', 'object', 'recognition', ';', 'use', 'large-scale', 'image', 'set', 'to', 'gain', 'sociological', 'insight', ';', 'conduct', 'audit', 'of', 'biased', 'facial', 'recognition', 'system', 'which', 'have', 'influence', 'real-world', 'regulation', ';', 'design', 'standard', 'and', 'process', 'to', 'mitigate', 'ethical', 'issue', 'with', 'datasets', 'and', 'model', ';', 'develop', 'a', 'framework', 'of', 'algorithmic', 'audit', 'for', 'AI', 'accountability', ';', 'and', 'more', '.']\n",
            "\n",
            "Original       : ['Many', 'of', 'her', 'paper', 'have', 'been', 'cited', 'hundred', 'of', 'time', '.']\n",
            "with POS       : ['Many', 'of', 'her', 'paper', 'have', 'be', 'cite', 'hundred', 'of', 'time', '.']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZPNyI9-ZdX"
      },
      "source": [
        "Sekilas terlihat bahwa dengan menggunakan POS tag, hasil lemmatisasi relatif lebih baik. Hal ini wajar karena secara default bila kita tidak memberitahukan informasi soal POS tag dari suatu kata, NLTK akan mengasumsikan bahwa kata tersebut adalah kata benda. Sedangkan bila kita memberitahu bahwa kata tersebut adalah kata kerja maka lemmatizer akan bisa memberikan hasil yang sesuai dengan konteks POS tersebut."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajU-y1qcr4go"
      },
      "source": [
        "## Phrase Chunking\r\n",
        "\r\n",
        "NLTK memiliki modul [`nltk.chunk`](https://www.nltk.org/api/nltk.chunk.html?highlight=chunk#module-nltk.chunk) yang pada dasarnya akan melakukan chunking berdasarkan [tag pattern](https://www.nltk.org/api/nltk.chunk.html?highlight=chunk#tag-patterns) yang diberikan. Tag pattern disini merupakan suatu modifikasi dari regex yang melakukan matching terhadap suatu urutan tags.\r\n",
        "\r\n",
        "Karena chunk parser ini hanya bisa melakukan chunking dari masukan berupa tree, maka perlu dilakukan tagging per kalimat menggunakan `pos_tag`. Karena bila menggunakan `pos_tag_sents` akan menghasilkan luaran berupa list bukan tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTS8t6e5A4cP",
        "outputId": "969749c7-2c10-42c7-c8ed-24331abfbd8e"
      },
      "source": [
        "from nltk.chunk import RegexpParser\r\n",
        "\r\n",
        "tag_pattern = r'NP: {<DT>?<JJ>*<NN>}'\r\n",
        "chunk_parser = RegexpParser(tag_pattern)\r\n",
        "\r\n",
        "chunked_phrase = [chunk_parser.parse(pos_tag(words)) for words in word_token]\r\n",
        "\r\n",
        "for sentence in chunked_phrase:\r\n",
        "  print(list(sentence))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Few', 'JJ'), ('researchers', 'NNS'), ('make', 'VBP'), ('breakthrough', 'JJ'), ('contributions', 'NNS'), ('to', 'TO'), ('even', 'VB'), Tree('NP', [('a', 'DT'), ('single', 'JJ'), ('field', 'NN')]), ('.', '.')]\n",
            "[('Fewer', 'JJR'), ('still', 'RB'), ('can', 'MD'), ('claim', 'VB'), ('to', 'TO'), ('have', 'VB'), ('made', 'VBN'), ('breakthrough', 'IN'), ('contributions', 'NNS'), ('to', 'TO'), ('multiple', 'VB'), ('fields', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('Timnit', 'NNP'), ('Gebru', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('those', 'DT'), ('few', 'JJ'), ('.', '.')]\n",
            "[('She', 'PRP'), ('has', 'VBZ'), ('worked', 'VBN'), ('on', 'IN'), Tree('NP', [('computer', 'NN')]), Tree('NP', [('vision', 'NN')]), ('problems', 'NNS'), ('in', 'IN'), Tree('NP', [('fine-grained', 'JJ'), ('object', 'JJ'), ('recognition', 'NN')]), (';', ':'), ('used', 'VBN'), Tree('NP', [('large-scale', 'JJ'), ('image', 'NN')]), ('sets', 'NNS'), ('to', 'TO'), ('gain', 'VB'), Tree('NP', [('sociological', 'JJ'), ('insight', 'NN')]), (';', ':'), ('conducted', 'VBN'), ('audits', 'NNS'), ('of', 'IN'), Tree('NP', [('biased', 'JJ'), ('facial', 'JJ'), ('recognition', 'NN')]), ('systems', 'NNS'), ('which', 'WDT'), ('have', 'VBP'), ('influenced', 'VBN'), Tree('NP', [('real-world', 'JJ'), ('regulation', 'NN')]), (';', ':'), ('designed', 'VBN'), ('standards', 'NNS'), ('and', 'CC'), ('processes', 'NNS'), ('to', 'TO'), ('mitigate', 'VB'), ('ethical', 'JJ'), ('issues', 'NNS'), ('with', 'IN'), ('datasets', 'NNS'), ('and', 'CC'), ('models', 'NNS'), (';', ':'), ('developed', 'VBD'), Tree('NP', [('a', 'DT'), ('framework', 'NN')]), ('of', 'IN'), ('algorithmic', 'JJ'), ('audits', 'NNS'), ('for', 'IN'), ('AI', 'NNP'), Tree('NP', [('accountability', 'NN')]), (';', ':'), ('and', 'CC'), ('more', 'RBR'), ('.', '.')]\n",
            "[('Many', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('papers', 'NNS'), ('have', 'VBP'), ('been', 'VBN'), ('cited', 'VBN'), ('hundreds', 'NNS'), ('of', 'IN'), ('times', 'NNS'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}